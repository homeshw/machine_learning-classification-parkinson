\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[table]{xcolor}


\title{Tuning Hyper Parameters in Neural Network Model for a Binary Classification}
\author{Homesh Wathsalya Ampe Mohottige \\z5314511 }
\date{5 October 2020}

\begin{document}

\maketitle
\newpage
\tableofcontents

\newpage
\section{Problem Definition}
\subsection{Abstract}
\subsection{Introduction}
Neural Networks has become a highly useful tool for classification problems\cite{JacquesEtienne} with minimal human interference. However still some manual intervention is needed by the user to tune the model with some of the given hyperparametes to gain an optimal solution for the problem presented. Here we are using a binary classification problem to demonstrate the solution
There are several hyper parameters that can be tuned for the given dataset.
A. Optimizer
B. Learning rate and momentum rate
C. Number of hidden layers
D. Different combinations of hidden neurons
\section{methodology}
A. Pre-processing dataset
Firstly, the the given classification data set will be visualized using histogram plots to identify the distribution of the data. According to the results, it will be decided whether its necessary to standardise the data in the given data set.
B. Building neural network models
Keras library in Python will be used to apply Tensorflow for model building and executions. Models will be built with several configurations to find-out the most optimum model for the particular classification use case. Following table shows each model configuration that has been tested.

each model configuration will run 10 times and following metrics will be recorded in each run. For each of these metrics, mean and 90th percentile will be calculated for each model configuration.

Optimizer performance Adam SGD
Learning and Momentum Rate
Hidden Layers 1,2,3,4
Different combinations of hidden layers

\begin{table}[htbp]
\begin{center}
\begin{tabular}{ |p{1cm}|c|p{1.5cm}|p{1.5cm}|c|p{2cm}| } 
\hline
Test Case & Optimizer & Learning Rate & Momentum Rate & Hidden Layers & Hidden Layer combinations\\
\hline
1 & \cellcolor{black!10}Adam & 0.01 & - & 1 & 25 \\ 
& \cellcolor{black!10}SGD & 0.01 & - & 1 & 25 \\ 
\hline
2 & Adam & \cellcolor{black!10}0.01 & - & 1 & 25 \\ 
& Adam & \cellcolor{black!10}0.001 & - & 1 & 25 \\ 
& Adam & \cellcolor{black!10}0.1 & - & 1 & 25 \\ 
& SGD & 0.01 & \cellcolor{black!10}0.5 & 1 & 25 \\ 
& SGD & 0.01 & \cellcolor{black!10}0.75 & 1 & 25 \\ 
& SGD & 0.01 & \cellcolor{black!10}0.9 & 1 & 25 \\ 
\hline
3 & Adam & 0.01 & - & \cellcolor{black!10}1 & 25 \\ 
& Adam & 0.01 & - & \cellcolor{black!10}2 & 25,25 \\ 
& Adam & 0.01 & - & \cellcolor{black!10}3 & 25,25,25 \\
& Adam & 0.01 & - & \cellcolor{black!10}4 & 25,25,25,25 \\
\hline
3 & Adam & 0.01 & - & 4 & \cellcolor{black!10}5,5,5,5 \\ 
& Adam & 0.01 & - & 4 & \cellcolor{black!10}10,10,10,10 \\ 
& Adam & 0.01 & - & 4 & \cellcolor{black!10}15,15,15,15 \\
& Adam & 0.01 & - & 4 & \cellcolor{black!10}20,20,20,20 \\
& Adam & 0.01 & - & 4 & \cellcolor{black!10}25,25,25,25 \\
\hline
\end{tabular}
\caption{Test Cases}
\label{tab:testcases}
\end{center}
\end{table}

And following matrices has been tested \\
- Train accuracy \\
- Validation Accuracy \\
- Precision \\
- Recall \\
- Elapsed time \\
\section{results}

Following shows the results of each test case and test scenario
\subsection{Optimizer}

below table shows performance matrices generated by models which used Adam and SGD as optimizers 
\begin{table}[h]
\begin{center}
\begin{tabular}{ |c|c|p{1.5cm}|p{2cm}|} 
\hline
Optimizer & Metric Name & Metric Mean & Metric 90th Percentile\\
\hline
Adam & Train Accuracy & 0.94 & 0.98\\ 
& Validation Accuracy & 0.68 & 0.69\\ 
& Precision & 0.93 & 0.98 \\ 
& Recall & 0.93 & 0.98 \\ 
& Elapsed Time (s) & 6.36 & 9.30 \\
\hline
SGD & Training Accuracy & 0.72 & 0.76\\ 
& Validation Accuracy & 0.65 & 0.68 \\ 
& Precision & 0.70 & 0.73 \\ 
& Recall & 0.73 & 0.80 \\ 
& Elapsed Time (s) & 5.16 & 6.57 \\
\hline
\end{tabular}
\caption{Results from Different Optimizers}
\label{tab:results_opt}
\end{center}
\end{table}
\section{conclusion}

\subsection{learning rate and momentum rate}

below table shows performance matrices generated by models which used different values for learning rate 
\begin{table}[h]
\begin{center}
\begin{tabular}{ |c|c|p{1.5cm}|p{2cm}|} 
\hline
Learning Rate & Metric Name & Metric Mean & Metric 90th Percentile\\
\hline
0.1 & Train Accuracy & 0.80 & 0.82\\ 
& Validation Accuracy & 0.63 & 0.65\\ 
& Precision & 0.73 & 0.76 \\ 
& Recall & 0.92 & 0.97 \\ 
& Elapsed Time (s) & 5.04 & 5.57 \\
\hline
0.01 & Train Accuracy & 0.93 & 0.99\\ 
& Validation Accuracy & 0.67 & 0.68\\ 
& Precision & 0.92 & 0.99 \\ 
& Recall & 0.94 & 0.99 \\ 
& Elapsed Time (s) & 5.17 & 8.11 \\
\hline
0.001 & Training Accuracy & 0.84 & 0.89\\ 
& Validation Accuracy & 0.65 & 0.67 \\ 
& Precision & 0.82 & 0.87 \\ 
& Recall & 0.86 & 0.90 \\ 
& Elapsed Time (s) & 4.82 & 5.79 \\
\hline
\end{tabular}
\caption{Results from Different Learning Rates}
\label{tab:results_learn}
\end{center}
\end{table}
\section{conclusion}

below table shows performance matrices generated by models which used different values for momentum rate 
\begin{table}[h]
\begin{center}
\begin{tabular}{ |c|c|p{1.5cm}|p{2cm}|} 
\hline
Momentum Rate & Metric Name & Metric Mean & Metric 90th Percentile\\
\hline
0.5 & Train Accuracy & 0.77 & 0.81\\ 
& Validation Accuracy & 0.67 & 0.68\\ 
& Precision & 0.75 & 0.80 \\ 
& Recall & 0.79 & 0.81 \\ 
& Elapsed Time (s) & 5.64 & 6.93 \\
\hline
0.75 & Training Accuracy & 0.83 & 0.90\\ 
& Validation Accuracy & 0.66 & 0.67 \\ 
& Precision & 0.81 & 0.90 \\ 
& Recall & 0.85 & 0.90 \\ 
& Elapsed Time (s) & 5.21 & 7.15 \\
\hline
0.9 & Training Accuracy & 0.89 & 0.96\\ 
& Validation Accuracy & 0.65 & 0.67 \\ 
& Precision & 0.88 & 0.95 \\ 
& Recall & 0.91 & 0.97 \\ 
& Elapsed Time (s) & 6.42 & 8.67 \\
\hline
\end{tabular}
\caption{Results from Different Momentum Rates}
\label{tab:results_momentum}
\end{center}
\end{table}
\section{conclusion}


\bibliographystyle{IEEEtran}
\bibliography{references}
\end{document}
